<div style="text-align: center;">
<br/>
<br/>
<br/>
<br/>

# 大数据系统开发实践实验报告

![alt text](image.png)

### 题　　目： 大数据系统开发实践

### 学　　院： 计算机学院

### 专业名称： 计算机科学与技术

### 小组成员：

成员1
成员2
成员3

### 任课教师： 郭贵锁

### 日　　期： 2024年9月14日

</div>
<div style="page-break-after: always;"></div>

## 1.搜索引擎的技术方案

### 功能描述

wkw负责写

### 技术选型

dyh负责

### 功能实现

yjx负责写

### 工作计划

dyh负责

### 组织结构

yjx负责写

### 软件质量保证

zjr负责

## 2.倒排索引的实现

### 实验要求

运用MapReduce算法计算,构建一个倒排索引, 将倒排索引存储在HBase中,数据为老师提供的sentence.txt文件。

### 实验进度

- 9.6-9.8 完成集群初步配置<br>
- 9.9-9.11 伪分布式和单机完全分布式集群启动成功<br>
- 9.12 完成倒排索引的代码编写<br>
- 9.13 完成三机完全分布式集群启动成功，单机完全分布式倒排运行成功<br>
- 9.14 三机完全分布式倒排运行成功

### 小组分工

每个人写自己的

### 数据准备

dyh负责写

### 环境的安装与配置

wkw负责写完全分布，yjx负责写伪分布

#### 伪分布

#### 完全分布

1. 节点分配设计

| 节点编号 | 节点类型          | IP地址       | 角色/职责                                    | CPU核数 | 内存(GB) | 磁盘空间(GB) | 备注                           |
| -------- | ----------------- | ------------ | -------------------------------------------- | ------- | -------- | ------------ | ------------------------------ |
| 1        | NameNode          | 192.168.3.81 | 管理HDFS元数据，负责文件系统的命名空间和控制 | 2       | 2        | 40           | 主要负责元数据存储与查询       |
| 2        | SecondaryNameNode | 192.168.3.83 | 协助NameNode备份元数据、检查点               | 8       | 2        | 40           | 作为NameNode的辅助节点         |
| 3        | DataNode          | 192.168.3.81 | 存储实际数据块，提供数据读写服务             | 2       | 2        | 40           | 每个DataNode存储HDFS数据块     |
| 4        | DataNode          | 192.168.3.82 | 存储实际数据块，提供数据读写服务             | 2       | 2        | 40           | 多个DataNode提供冗余和高可用性 |
| 5        | DataNode          | 192.168.3.83 | 存储实际数据块，提供数据读写服务             | 8       | 2        | 40           | 提供负载均衡和分布式存储       |
| 6        | ResourceManager   | 192.168.3.81 | 管理集群资源，调度计算任务                   | 2       | 2        | 40           | 负责资源调度                   |
| 7        | NodeManager       | 192.168.3.82 | 负责任务执行及资源管理                       | 2       | 2        | 40           | 管理YARN应用程序的容器         |
| 8        | NodeManager       | 192.168.3.83 | 负责任务执行及资源管理                       | 8       | 2        | 40           | 提供计算能力的分布式资源节点   |
| 9        | Zookeeper         | 192.168.3.83 | 提供分布式协调服务，管理集群节点状态         | 8       | 2        | 40           | 负责NameNode的高可用性和选举   |

2. 网络配置流程

- **设置IP地址**<br>
  需要修改主机VM8，虚拟网络编辑器和虚拟机网卡。<br>首先是VMnet8的设置：<br>![alt text](image-1.png)<br>
  将IP，网关和DNS的第三位调整一致。<br>
  然后设置虚拟网络编辑器：<br>
  ![alt text](image-2.png)<br>
  将子网的网段前三位改为与VMnet8的IP一致。<br>
  然后配置虚拟机网卡，在命令终端输入<br>
  
  ```
  vim /etc/sysconfig/network-scripts/ifcfg-ens33
  ```
  
  进入页面修改虚拟机的网卡如下图所示：<br>
  ![alt text](image-3.png)<br>
  至此，所有IP配置完毕
- **使用XShell连接**<br>
  如果可以使用XShell连接，说明本地的网络与虚拟机服务器之间的网络是畅通的，没有防火墙或其他网络障碍阻止连接。所以这步需要先关闭防火墙，确认访问端口开启，然后再用XShell连接，连接成功后ping外网和本机IP，保证网络畅通。<br>
  首先关闭防火墙，并检查防火墙状态:
  
  ```
  sudo systemctl stop firewalld
  sudo systemctl disable firewalld
  sudo systemctl status firewalld
  ```
  
  出现以下状态即为关闭成功：![alt text](image-4.png)<br>
  然后检查开放的端口：
  
  ```
  sudo ss -tuln
  ```
  
  检查端口是否开放<br>
  ![alt text](image-5.png)
  可以看到22号端口是开放的。<br>
  然后我们使用XShell对其进行连接：<br>
  ![alt text](image-6.png)图示即为连接成功。<br>
  然后测试ping网络：<br>
  ![alt text](image-7.png)<br>
  如图即为成功ping通。至此已经完成XShell的连接.<br>

3. 软件安装和虚拟机复制

- **jdk安装**
- **hadoop安装**
- **zooKeeper安装**
- **hbase安装**
- **虚拟机复制**
  将样本机复制两次样本，一共有三台虚拟机，命名为hadoop081，hadoop082，hadoop083：<br>
  ![alt text](image-8.png)<br>
  将两台主机的网卡分别改为对应的IP，再修改其主机名。修改IP之前已经展示，接下来就展示主机名修改。命令输入：
  
  ```
  vim /etc/hostname
  ```
  
  ![alt text](image-9.png)<br>
  如上图为hadoop082主机名的修改。

4. 节点分配文件

- **core-site.xml**
  
  ```
  <configuration>
      <property>
              <name>fs.defaultFS</name>
              <value>hdfs://hadoop081:9000</value>
      </property>
      <property>
              <name>hadoop.tmp.dir</name>
              <value>/var/big_data</value>
      </property>
  </configuration>
  ```
- **hdfs-site.xml**
  
  ```
  <configuration>
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>hdfs://hadoop083:9868</value>
        </property>
    </configuration>
  ```
- **yarn-site.xml**
  
  ```
  <configuration>
      <property>
              <name>yarn.nodemanager.aux-services</name>
              <value>mapreduce_shuffle</value>
      </property>
      <property>
              <name>yarn.resourcemanager.hostname</name>
              <value>hadoop081</value>
      </property>
      <property>
              <name>yarn.scheduler.minimum-allocation-mb</name>
              <value>256</value>
       </property>
       <property>
              <name>yarn.scheduler.maximum-allocation-mb</name>
              <value>1540</value>
       </property>
       <property>
              <name>yarn.nodemanager.resource.memory-mb</name>
              <value>1540</value>
      </property>
      <property>
               <name>yarn.nodemanager.env-whitelist</name><value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
      </property>
  </configuration>
  ```
- **mapred-site.xml**
  
  ```
  <configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
  </configuration>
  '''
  ```
- **hbase-site.xml**
  
  ```
  <configuration>
      <property>
          <name>hbase.cluster.distributed</name>
          <value>true</value>
      </property>
      <property>
          <name>hbase.tmp.dir</name>
          <value>/opt/module/hbase-2.5.4/tmp</value>
      </property>
      <property>
          <name>hbase.unsafe.stream.capability.enforce</name>
          <value>false</value>
      </property>
      <property>
          <name>hbase.rootdir</name>
          <value>hdfs://hadoop081:9000/hbase</value>
      </property>
      <property>
          <name>hbase.zookeeper.quorum</name>
          <value>hadoop081,hadoop082,hadoop083</value>
      </property>
      <property>
          <name>hbase.zookeeper.property.dataDir</name>
          <value>/opt/module/zookeeper-3.8.2/zkData</value>
      </property>
      <property>
          <name>hbase.master.info.port</name>
          <value>16010</value>
      </property>
  </configuration>
  ```
- **zoo.cfg**
  
  ```
  # The number of milliseconds of each tick
  tickTime=2000
  # The number of ticks that the initial 
  # synchronization phase can take
  initLimit=10
  # The number of ticks that can pass between 
  # sending a request and getting an acknowledgement
  syncLimit=5
  # the directory where the snapshot is stored.
  # do not use /tmp for storage, /tmp here is just 
  # example sakes.
  dataDir=/opt/module/zookeeper-3.8.2/zkData
  # the port at which the clients will connect
  clientPort=2181
  # the maximum number of client connections.
  # increase this if you need to handle more clients
  #maxClientCnxns=60
  #
  # Be sure to read the maintenance section of the 
  # administrator guide before turning on autopurge.
  #
  # https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
  #
  # The number of snapshots to retain in dataDir
  #autopurge.snapRetainCount=3
  # Purge task interval in hours
  # Set to "0" to disable auto purge feature
  #autopurge.purgeInterval=1
  
  ## Metrics Providers
  #
  # https://prometheus.io Metrics Exporter
  #metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider
  #metricsProvider.httpHost=0.0.0.0
  #metricsProvider.httpPort=7000
  #metricsProvider.exportJvmInfo=true
  server.1=hadoop081:2888:3888
  server.2=hadoop082:2888:3888
  server.3=hadoop083:2888:3888
  ```

5. 单机完全分布集群启动
6. 三机完全分布集群网络配置
7. 三机完全分布集群启动

### 算法及实现

zjr负责写

> &emsp;&emsp;MapReduce是由hadoop提供的一个开源软件框架，基于该框架能够容易地编写应用程序运行在由上千个商用机器组成的大集群上，并以一种可靠的，具有容错能力的方式并行地处理上TB级别的海量数据集。

&emsp;&emsp;在本次实验中，我们使用MapReduce框架实现了词频统计及倒排索引的Java程序，能够在hadoop上分布式运行，并将处理结果导入进hbase当中。
&emsp;&emsp;Java项目使用jdk1.8.0_422。采用Maven自动导入hadoop上MapReduce框架的相关依赖，pom.xml配置文件如下：

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.test</groupId>
    <artifactId>InvertedMapReduce</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>3.3.5</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>3.3.5</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-common</artifactId>
            <version>2.5.10</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.5.10</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-mapreduce</artifactId>
            <version>2.5.10</version>
        </dependency>
    </dependencies>

</project>
```

&emsp;&emsp;算法的具体实现如下：

1. Mapper阶段实现代码及算法说明
   - **InvertedMapper.java代码**
     ```
     
     ```
   - **类与数据类型**
   - **算法核心**

### 运行结果与分析

wkw负责写

### 总结（心得、体会）

每个人写一点

